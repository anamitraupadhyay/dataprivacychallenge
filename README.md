Script1.py is more documented approach of script.py but same.
-------------------------------------------------------------
well before arriving to this approach i did 2 others, first one was matching 2 csvs using the concept of normalized score based matching and its flaws were only works on predictable variation of like +_10% per se, then i followed an approach of 2 steps including first segregating k umber of clusters based on cumulative ratios and similar to this approaches feature engineering part "burn rate" then matching based off of scores being high allotted to respective matches, its issue was it has to fail in randomized pattern of noise addition and failed to detect trends like if its a student his education spending is more and if its a retiree then its zero like these sort of. Now in this approach we took a detailed well balanced approach first we bring all the components to 0-1 scaling using min-max scaling to remove feature size biases and to bring protected values into same range, then applied principal component fit on these min-maxed values to align on behavioral axes or trends per se and embed protected rows in same pattern derived from original since we are learning the trends from original and applying it on protected, then to understand correlation between behaviours we apply covariance matrix and then inverse it to prepare for correlation aware distance measurement where we used mahalanobis instead of euclidean distance where we matched the protected rows by behaviour repsecting correlations then taking top k number matches here 3 to find best candidate between those and as 3 giving us space to choose as top k mathcing retrieves most behaviorally similar original records. And according to me why this approach is scientifically robust: handles features correlation, denoises variation and in turn works where trends are preserved in +_10% , point to be noted am talking about trend instead of actual data variating on which my earliest idea was based on.
